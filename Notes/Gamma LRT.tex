\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm} 
\usepackage{xcolor}     % For the \textcolor{}{} command
\usepackage{mathrsfs}   % For the \mathscr{} font

\usepackage{natbib} % For citation functions like \citet and \citep

\usepackage{tikz}   % For graphing
\usepackage{pgfplots}   % For graphing a function

\usepackage{subcaption} % For sub-figures


\newcommand{\bG}{\mathbb{G}}
\newcommand{\bF}{\mathbb{F}}
\newcommand{\bP}{\mathbb{P}}
\newcommand{\bR}{\mathbb{R}}

\newcommand{\sW}{\mathscr{W}}
\newcommand{\swe}{\mathscr{W}(\eta)}

\newcommand{\dfdg}{d \bF / d \bG}
\newcommand{\dfdgfrac}{\frac{d \bF}{d \bG}}


\title{Gamma LRT}
\date{}


\begin{document}

\maketitle

This document describes the steps to obtain the tail function for the importance sampling weights in Gamma distribution example. 
We use an Exponential distribution as the proposal dist with pdf $g(x)$, and our target distribution is Gamma with pdf $f(x)$.

Steps are as follows (I think!)
\textcolor{blue}{This looks really good Payman. I just added a couple notes in blue.}

Step 1. Set the target distribution, $f(x)$ as a Gamma distribution with $\alpha$ as shape and $\beta$ as scale:

\begin{align}
    f(x;\alpha,\beta) = \frac{\beta^{\alpha}}{\Gamma(\alpha)} x^{\alpha-1} e^{\beta x}
\end{align}

Step 2. Set the proposal distribution, $g(x)$, as an Exponential dist with scale parameter of one:
\begin{align}
    g(x) = e^{-x}
\end{align}
\textcolor{blue}{More generally, we can use any scale parameter for the exponential. For the purposes of this discussion, we just need the scale parameter to be fixed. I think it's pretty reasonable to keep $\lambda=1$ for now to avoid cluttering our formulas, although eventually we'll need to do the general case.}\\

Step 3. The pdf of weights is $w(x;\alpha,\beta) = \frac{f(x;\alpha,\beta)}{g(x)}$. \textcolor{blue}{This actually isn't quite the pdf of the weights. Instead, the $X$s are iid from $g$, and the weights are obtained by evaluating $w$ at the simulated $X$s. The distribution on the weights is then induced by the distribution on $X$ and the functional relationship between $X$ and $W$. If the words ``pushforward measure'' help clarify things, that's exactly what's happening here. If not, $W$ is a transformation of $X$. In principle, we can get the pdf of $W$ by computing the Jacobian of the (inverse) transformation (i.e. the derivative of the inverse of the likelihood ratio function). Personally, I prefer to work directly with the CDF or survival function; for one, I think it's easier, and for two, we need the survival function for step 4 anyway.}\\

Step 4. Use the pdf in step 3 and compute its tail function as described in Pickands paper:

\begin{align}
    P(x) = \frac{F(X \geq u+x)}{F(X \geq u)}
\end{align}

where $F(.)$ is the CDF of weights as defined in step 3.

Step 5. Find $Q_3$, and median as described in the paper. Compute the $c$ and $a$. 


\section{William's Derivation}

\textcolor{red}{I found a few mistakes. See the git log for changes. I still think my new answer is wrong, but at least it's a bit more reasonable. I'll make another pass at some point and see if I notice more mistakes.}

I'm going to mostly stick to the above notation, but I would like to use the general exponential distribution until I find myself needing to set $\lambda=1$. I will also suppress dependence on parameters in the various densities to avoid extra typing. Thus, $g(x) = \exp(-x/\lambda)/\lambda$.

At a high-level, my plan is to compute the survival function of the weights, as well as their density and the derivative of their density. I can then use Equation (1.1.30) on page 15 of \citet{deH06} to (hopefully) compute the tail index. There are enough steps here that it's going to take a few days' work. One step at a time.

To start, we note that the weight, $W$, can be written as a random variable in the following way.
%
\begin{align}
    W(X) &= \frac{f(X)}{g(X)}\\
    &= \frac{\lambda}{\beta^\alpha \Gamma(\alpha)} X^{\alpha - 1} \exp\left[ -X \left( \frac{1}{\beta} + \frac{1}{\lambda} \right) \right] \label{eq:def_W_wrong}\\
    &=: c X^{\alpha - 1} \exp(-X / \gamma)
\end{align}
%
where $c = \lambda / \beta^\alpha \Gamma(\alpha)$, and $\gamma = (1/\beta + 1/\lambda)^{-1}$. I doubt that it will every be relevant, but I think it's cute that $\gamma$ is a harmonic mean. 

We can now work out the survival function of $W$. We write $\sW$ for the inconveniently named Lambert-W function, which is the inverse function of $x \mapsto x e^x$. I.e. $\sW$ satisfies $\sW(x) \exp[\sW(x)] = x$.
%
\begin{align}
    \bP (W > w) &= \bP \left( X^{\alpha - 1} e^{-X/\gamma} > \frac{w}{c} \right) \label{eq:W_surv1}\\
    &= \bP \left( X > - (\alpha - 1) \gamma \cdot \sW \left[ - \frac{(w/c)^{1/(\alpha-1)}}{(\alpha - 1) \gamma} \right] \right) \label{eq:W_surv2}\\
    &= \exp \left( \frac{(\alpha - 1) \gamma}{\lambda} \cdot \sW \left[ - \frac{(w/c)^{1/(\alpha-1)}}{(\alpha - 1) \gamma} \right] \right) \label{eq:W_surv3}
\end{align}
%
where we pass from (\ref{eq:W_surv1}) to (\ref{eq:W_surv2}) with the help of Wolfram Alpha.

\subsection{PDF}
\label{sec:pdf}

Next, we need the PDF of $W$, and later its derivative. I'm going to find it easier to do this in stages. Let $\bar{\varphi}(w) = \bP(W > w)$ and $\eta$ be the argument to $\sW$ in (\ref{eq:W_surv3}). The (negative) PDF of $w$ is obtained by differentiating $\bar{\varphi}$. Note that the derivative of $\sW$ can be obtained implicitly from its\footnote{Usually, I try not to use ``it'' to refer to anything in academic writing (Tom drilled this into me during my master's thesis revisions). Here however, I think it's a pretty good fit and doing anything else would be very wordy.} defining equation
%
\begin{align}
    \nabla_w \bar{\varphi}(w) &=  \frac{(\alpha - 1) \gamma}{\lambda} \bar{\varphi}(w) \nabla_w \sW (\eta)\\
    \nabla_w \sW(\eta) &= \frac{\sW(\eta)}{\eta [ 1 + \sW(\eta)]} \nabla_w \eta\\
    \nabla_w \eta &= - \frac{1}{c \gamma (\alpha - 1)^2} \left( \frac{w}{c} \right)^{\frac{1}{\alpha - 1} - 1} = \frac{\eta}{w(\alpha - 1)} \label{eq:nabla_eta}
\end{align}
%
Putting this all together, and after a surprising amount of cancellation, I get
%
\begin{align}
    f_W(w) &:= -\nabla_w \bar{\varphi}(w)\\
    &= - \bar{\varphi}(w) \frac{\gamma}{\lambda w} \cdot \frac{\sW(\eta)}{1 + \sW(\eta)}
\end{align}

\subsection{Derivative of 1 over Hazard Rate}

We return now to Theorem 1.1.8 of \citet{deH06}, specifically expression (1.1.29)\footnote{Note that, in survival analysis, the hazard rate of a distribution is defined as the density over the survival function. The quantity being differentiated in (1.1.29) before taking the limit is thus the reciprocal of the hazard rate of $W$}. We first compute the survival function of $W$ over its density, wherein we get some nice cancellation.
%
\begin{align}
    \frac{\bar{\varphi}(w)}{f_W(w)} &= - \frac{\lambda w}{\gamma} \cdot \frac{1 + \sW(\eta)}{\sW(\eta)} \label{eq:recip_hazard}
\end{align}
%
where $\sW$ and $\eta$ are defined in Section \ref{sec:pdf}. We now need to differentiate the expression in \ref{eq:recip_hazard}. Fortunately, we can recycle identities (\ref{eq:W_surv2}) and (\ref{eq:W_surv3}). We first present some identities which help with simplification, then proceed with computing the derivative.
%
\begin{align}
    \nabla_w \sW(\eta) &= \partial \sW(\eta) \cdot \partial \eta\\
    &= \frac{1}{w(\alpha - 1)} \cdot \frac{\sW(\eta)}{ 1 + \sW(\eta)}\\
    \nabla_w \frac{1 + \sW(\eta)}{\sW(\eta)} &= \nabla_w \frac{1}{\swe}\\
    &= \frac{-\nabla_w \swe}{\swe^2}\\
    &= - \frac{1}{w (\alpha - 1) \swe (1 + \swe)}\\
    \nabla_w \frac{\bar{\varphi}(w)}{f_W(w)} &= - \frac{\lambda}{\gamma} \left[ \frac{1 + \swe}{\swe} + w \nabla_w \frac{1 + \sW(\eta)}{\sW(\eta)} \right]\\
    &= \frac{\lambda}{\gamma} \left[ \frac{1}{(\alpha - 1) \swe (1 + \swe)} - \frac{1 + \swe}{\swe} \right] \label{eq:grad_recip_hazard}
\end{align}
%
All that remains is to compute the limit of (\ref{eq:grad_recip_hazard}) as $w \uparrow \infty$. Note that $\eta \rightarrow - \infty$ as $w \rightarrow \infty$, so $\sW(\eta) \rightarrow - \infty$ as well. Re-write (\ref{eq:grad_recip_hazard}) as $(\lambda / \gamma) [ A - B]$. We now explore the asymptotic behaviour of $A$ and $B$. This is actually pretty straightforward: $A \rightarrow 0$ and $B \rightarrow 1$. Thus, the tail index, $\nabla_w [\bar{\varphi}(w) / f_W(w)]$, goes to $- \lambda / \gamma$ as $w \uparrow \infty$. Expanding this ratio, we get
%
\begin{align}
    - \frac{\lambda}{\gamma} &= - \frac{\lambda}{(1/\beta + 1/\lambda)^{-1}}\\
    &= - \left( 1 + \frac{\lambda}{\beta} \right)
\end{align}

\subsection{Discussion}

This final conclusion, while pleasantly simple, cannot possibly be correct. Surely, the tail index must depend on both parameters of the target distribution. Frankly, an incorrect answer isn't terribly surprising here. It's a long derivation with lots of room for mistakes. If I have time between now and our next meeting, I'll spend some time reviewing my work. If not, I would like to go over it again with you Payman, both to explain my reasoning and to double-check my work.


\section{Second Attempt}

There are a few mistakes in the above analysis. First, a typo. In the definition of $W$, the exponent should contain the difference between $1/\beta$ and $1/\lambda$ rather than their sum (see Equation (\ref{eq:def_W_wrong})). Fortunately, we just set this sum equal to $1/\gamma$ in the rest of our analysis, so we simply re-refine $\gamma = (\beta^{-1} - \lambda^{-1})^{-1}$ and the rest of our calculations hold (insofar as the rest of the calculations are themselves correct).

The second error is more egregious. In moving from equation (\ref{eq:W_surv1}) to (\ref{eq:W_surv2}), we used the Lambert-W function, $\sW$, to invert a function of the form $x \mapsto ax e^{-bx}$, with $a,b>0$. However, in part of its domain, $\sW$ is multi-valued. I find it easiest to understand this behaviour geometrically. Consider the simplest function of this form, $x e^{-x}$. Figure \ref{fig:x_ex} shows this function, as well as a horizontal line at $Y=0.2$. Note that this line intersects the curve at two points, so there are two solutions to the equation $x e^{-x} = 0.2$. In fact, this will be true for any positive value of $y$ below the maximum. We will prove this fact later, but for now take it as motivation for investigating the multi-valuedness of $\sW$.

\begin{figure}
    \centering
    % \resizebox{\columnwidth}{!}{%
    \begin{tikzpicture}
        \begin{axis}[xlabel=$X$, ylabel=$X e^{-X}$,domain=0:5,samples=50]
            \addplot[blue] (x,x/e^x);
            \addplot[red] (x, 0.2);
        \end{axis}
    \end{tikzpicture}
    % }
    \caption{Graph of $y = x e^{-x}$, with a horizontal line at $y=0.2$.}
    \label{fig:x_ex}
\end{figure}

The ultimate reason that we care about the multi-valuedness of $\sW$ is that the set $\{ W(X) > w \}$ corresponds to an interval in $X$. Thus, the survival function of $W$ is computed as a difference in the survival function of X at two points. We now proceed with fixing our derivation of the tail index for $W$.

To start, note that $W$ is defined as the likelihood ration between the $\mathrm{Gamma}(\alpha, \beta)$ and $\mathrm{Exp}(\lambda)$ distributions, evaluated at a random variable with the matching exponential distribution. 
%
\begin{align}
    W(X) &= \frac{\lambda}{\beta^\alpha \Gamma(\alpha)} X^{\alpha - 1} \exp\left[ -X \left( \frac{1}{\beta} - \frac{1}{\lambda} \right) \right]\\
    &=: c X^{\alpha - 1} \exp(-X / \gamma) \label{eq:W_of_X}
\end{align}
%
Next, we need the survival function of $W$.
%
\begin{align}
    \bP(W > w) &= \bP \left\{ X^{\alpha - 1} \exp(-X / \gamma) > \frac{w}{c} \right\} \label{eq:W_surv_raw}
\end{align}
%
We now present some properties of the function $h(x) = x^a e^{-bx}$, which we will then use to simplify Expression (\ref{eq:W_surv_raw}).

\subsection{On the function $h(x) = x^a \cdot e^{-bx}$}

To start, $\lim_{x \rightarrow \infty} h(x) = 0$ and $h(0) = 0$ (we will not concern ourselves with negative arguments of $h$). We can maximize $h$ by differentiating and solving for a root. This is easier if we instead work with $\log h$.
%
\begin{align}
    \nabla \log h(x) &= \nabla \left[ a \log(x) - b x \right]\\
    &= \frac{a}{x} - b \label{eq:grad_log_h}
\end{align}
%
This expression has a unique root at $x_* = a/b$. The second derivative test trivially confirms that $x_*$ is a maximizer. Evaluating $h(x_*) = (a/eb)^a=: y_*$ gives us an upper bound for $h$ (yes, this bound does involve multiplication by $e$).

Next, we attempt to invert $h$. We will proceed formally, ignoring any problems which can arise from raising negative numbers to arbitrary powers, and verify that the inverse we end up with is well-defined. Later, we will verify that we have, in fact, found all inverses of $h$ for the parameter values we consider. Our derivation of the inverse will ultimately involve the Lambert-W function\footnote{The Lambert-W function, $\sW$, is defined as the inverse of the function $z \mapsto ze^z$. That is, $\sW(z) \exp[\sW(z)] = z$. Note that $\sW$ is multi-valued on (part of) $\bR$. The ``principal branch'', denoted $\sW_0$, is defined for $z \geq -1/e$. The second branch, denoted $\sW_{-1}$, is defined only for $-1/e \leq z < 0$. I don't have a great understanding of how multi-valued functions on $\mathbb{C}$ work, so I'm going to take a pragmatic approach. For appropriate values of $z$, I denote the two branches $\sW_l$ and $\sW_u$, with the convention that $\sW_l \leq \sW_u$.}; the rest of the details revolve around setting up its application.
%
\begin{align}
    x^a e^{-bx} &= y\\
    x e^{-bx/a} &= y^{1/a}\\
    -\frac{bx}{a} e^{-bx/a} &= -\frac{b}{a} \cdot y^{1/a} \label{eq:h_inv1}\\
    -\frac{bx}{a} &= \sW\left( -\frac{b}{a} \cdot y^{1/a} \right) \label{eq:h_inv2}\\
    x &= -\frac{a}{b} \cdot \sW\left( -\frac{b}{a} \cdot y^{1/a} \right) = h^{-1}(y) \label{eq:h_inv_form}
\end{align}
%
A few notes. First, our final expression for $h^{-1}$ is perfectly well-defined. The only possible problem is that we could have lost one or more roots along the way. Second, in moving from (\ref{eq:h_inv1}) to (\ref{eq:h_inv2}), we have applied the $\sW$ function without checking whether it is well-defined. Doing so now, we note that $\sW(x)$ is not defined for $x < -e^{-1}$, has two real values for $-e^{-1} \leq x < 0$ (although these roots coincide for $x = -e^{-1}$), and one real value for $x \geq 0$. In the context of our $h$ function, these conditions correspond to $y > (a/eb)^a = y_*$, $y_* \geq y >0$, and $y \leq 0$ respectively. Note that solutions for $y < 0$ are negative in $x$, so are disregarded. These algebraic possibilities correspond exactly with the geometry in Figure \ref{fig:x_ex}. Above $y_*$ or below $0$ there are no inverses, below $y_*$ but above $0$ there are two inverses, and at $0$ or $y_*$ there are unique inverses. 

While the above analysis is nice, it does still remain to check that we haven't missed any inverses of $h$. To this end, observe that the function $h$ is strictly monotonic both below and above $x_*$ (increasing and decreasing respectively). This can be checked easily by inspection of Equation (\ref{eq:grad_log_h}). Thus, there is at most one solution of the equation $h(x) = c$ on each of the intervals $(-\infty, x_*]$ and $[x_*, \infty)$. We have already identified all such solutions.

In summary, we have completely characterized the inverses of the function $h(x) = x^a e^{-bx}$ for $x\geq 0$ and $a, b > 0$. These are given by Equation (\ref{eq:h_inv_form}), possibly with two distinct values of $\sW$. In practice, when writing code to evaluate $h^{-1}(y)$, we can check whether $y > y_*$ or $y < 0$ (no solution), $0 < y < y_*$ (two solutions), or $y \in \{ 0, y_* \}$ (one solution), where $y_* = (a/eb)^a$.

\textcolor{red}{It would be nice to use proper names for the two branches of $\sW$. I think one is called the ``principal'' branch, but I will need to confirm. Also do this in the following section.}

\subsection{Computing the Tail Index of $W$}

We now return to the problem of computing $\bP(W > w)$. Recalling Equation (\ref{eq:W_surv_raw}) and applying our inverse formula given in Equation \ref{eq:h_inv_form}, this probability is
%
\begin{align}
    &\bP \left\{ X^{\alpha - 1} \exp(-X / \gamma) > \frac{w}{c} \right\} \\
    &=\bP \left\{ -\gamma (\alpha - 1) \sW_u \left[ - \frac{1}{\gamma (\alpha - 1)} \left( \frac{w}{c} \right)^{1/(\alpha - 1)} \right] < X  \right. \\
    &\left. < -\gamma (\alpha - 1) \sW_l \left[ - \frac{1}{\gamma (\alpha - 1)} \left( \frac{w}{c} \right)^{1/(\alpha - 1)} \right]\right\}\\
    &=: \bP \left\{ X_l(w) < X < X_u(w) \right\}
\end{align}
%
where $\sW_l \leq \sW_u$ are two (possibly equal) real-valued branches of the Lambert-W function. Finally, since $X \sim \mathrm{Exp}(\lambda)$, the survival function of $W$ is $\bP(W > w) = \exp(-X_l(w)/ \gamma) - \exp(-X_u(w)/\gamma)$ (\textcolor{red}{note the order of $X_l$ and $X_u$. I still need to fix this below}).

Recall that the function $h$ has an upper bound, $h \leq (a / eb)^a$. Applying this bound to our expression for $W$, Equation \eqref{eq:W_of_X}, we get
\begin{align}
W &\leq c \left[ \frac{\gamma (\alpha - 1)}{e} \right]^{\alpha - 1}\\
&:= W_* \nonumber
\end{align}

Having established the survival function of $W$, we now need to compute its density (i.e. the negative derivative of the survival function; this is going to get messy). We then need to take their ratio, differentiate, and take the limit as $w \rightarrow w_*$. At some point, we may need to start working with numerical derivatives, but I would like to do as much analytically as is reasonably possible.

\subsection{Density of $W$ - try 1}

We start by the survival function that we just established earlier:

\begin{align}
\bP(W > w) = \exp(-X_u(w)/ \gamma) - \exp(-X_l(w)/\gamma)
\end{align}

To compute the density, we need to find the negative partial derivative w.r.t $w$. This means: 

\begin{align}
f(w) = - \frac{\partial}{\partial w} Pr(W \ge w) = \frac{\partial}{\partial w} \exp(-X_l(w)/ \gamma) - \exp(-X_u(w)/\gamma)
\end{align}

Let's start with:

\begin{align}
\frac{\partial}{\partial w} \exp \left( \frac{-X_{l}(w)}{\gamma} \right) = \frac{-1}{\gamma} \frac{\partial}{\partial w}(X_{l}(w)) \exp(\frac{-X_{l}(w)}{\gamma})
\end{align}

We note that:

\begin{align}
\frac{\partial}{\partial w}(X_{l}(w)) &= \frac{\partial}{\partial w} \left( -\gamma (\alpha - 1) \sW_l \left( \frac{-1}{\gamma (\alpha-1)} \left(\frac{w}{c}\right)^{\frac{1}{\alpha-1}} \right) \right) \\
&= \gamma (\alpha - 1) \frac{\partial}{\partial w} \left( \sW_l(\frac{-1}{\gamma (\alpha-1)} \frac{1}{c^{\alpha-1}} w^{\frac{1}{\alpha-1}})  \right) \\
&= -\gamma (\alpha-1) \frac{\partial}{\partial w} \left( \sW_l(A w^{\frac{1}{\alpha-1}}) \right)
\end{align}

and we used the notation of $A = \frac{-1}{\gamma (\alpha-1)} \frac{1}{c^{\alpha-1}}$ for convinience. Recall that we already established in equation (11) of this document that:

\begin{align}
\frac{\partial}{\partial x} \sW(u) = \frac{\sW(u)}{ u (1+\sW(u)) } \frac{\partial u}{\partial x}
\end{align}
    
Therefore we can continue in equation 44 as:

\begin{align}
\frac{\partial}{\partial w}(X_{l}(w)) &= -\gamma (\alpha-1) \frac{\sW_l(A w^{ \frac{1}{\alpha-1} })}{ A w^{\frac{1}{\alpha-1}} \left( 1 + \sW_l(A w^{ \frac{1}{\alpha-1} }) \right) } \frac{\partial}{\partial w}(A w^{\frac{1}{\alpha-1}}) \\
&= -\gamma (\alpha-1) \frac{\sW_l(A w^{ \frac{1}{\alpha-1} })}{ A w^{\frac{1}{\alpha-1}} \left( 1 + \sW_l(A w^{ \frac{1}{\alpha-1} }) \right) } A \frac{1}{\alpha-1} w^{\frac{2-\alpha}{\alpha-1}} \\
&= -\gamma \frac{\sW_l(A w^{ \frac{1}{\alpha-1} })}{ \left( 1 + \sW_l(A w^{ \frac{1}{\alpha-1} }) \right)} \frac{1}{w}
\end{align}
        
So we can write: 

\begin{align}
\frac{\partial}{\partial w} \exp \left( \frac{-X_{l}(w)}{\gamma} \right) &= \frac{-1}{\gamma} \frac{\partial}{\partial w}(X_{l}(w)) \exp(\frac{-X_{l}(w)}{\gamma}) \\
&= \frac{-1}{\gamma} \left( -\gamma \frac{\sW_l(A w^{ \frac{1}{\alpha-1} })}{ \left( 1 + \sW_l(A w^{ \frac{1}{\alpha-1} }) \right)} \frac{1}{w} \right) \exp(\frac{-X_{l}(w)}{\gamma}) \\
&= \frac{\sW_l(A w^{ \frac{1}{\alpha-1} })}{ \left( 1 + \sW_l(A w^{ \frac{1}{\alpha-1} }) \right)} \frac{1}{w} \exp(\frac{-X_{l}(w)}{\gamma}) 
\end{align}
    
With a similar work, we can derive:

\begin{align}
\frac{\partial}{\partial w} \exp \left( \frac{-X_{u}(w)}{\gamma} \right) = \frac{\sW_u(A w^{ \frac{1}{\alpha-1} })}{ \left( 1 + \sW_u(A w^{ \frac{1}{\alpha-1} }) \right)} \frac{1}{w} \exp(\frac{-X_{u}(w)}{\gamma}) 
\end{align}
   
Finally we can derive the pdf of $w$ as follows: 

\begin{align}
f(w) &= \frac{\partial}{\partial w} \exp(-X_l(w)/ \gamma) - \frac{\partial}{\partial w} \exp(-X_u(w)/\gamma) \\
&= \frac{\sW_l(A w^{ \frac{1}{\alpha-1} })}{ \left( 1 + \sW_l(A w^{ \frac{1}{\alpha-1} }) \right)} \frac{1}{w} \exp(\frac{-X_{l}(w)}{\gamma}) - \frac{\sW_u(A w^{ \frac{1}{\alpha-1} })}{ \left( 1 + \sW_u(A w^{ \frac{1}{\alpha-1} }) \right)} \frac{1}{w} \exp(\frac{-X_{u}(w)}{\gamma})
\end{align}
        

\bibliographystyle{apalike}
\bibliography{MyBib}

\end{document}